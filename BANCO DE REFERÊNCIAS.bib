% Encoding: UTF-8

@Article{VORTEX2614,
  author     = {Rodrigo de Araujo e José Mauro Sandy e Elder José Cirilo e Flávio Luiz Schiavoni},
  journal    = {Revista Vórtex},
  title      = {Análise e classificação de Linguagens de Programação Musical},
  year       = {2018},
  issn       = {2317–9937},
  number     = {2},
  volume     = {6},
  abstract   = {As linguagens de programação musical datam dos primórdios da computação e sofreram - e ainda sofrem - uma grande influência da evolução e pesquisa na área de Linguagens de Programação.  Esta influência resultou em um ecossistema de linguagens com diferentes paradigmas, mas sob o mesmo domínio, a Computação Musical. Neste artigo, apresentamos as questões históricas da evolução destas linguagens, as suas questões técnicas e de desenvolvimento e também uma análise e avaliação das mesmas levando em consideração a facilidade de uso e critérios como legibilidade, expressividade e facilidade de escrita e também a influência da comunidade no desenvolvimento das mesmas. Por fim, apresentamos uma discussão sobre esta análise e avaliação que pode auxiliar artistas e programadores.},
  file       = {:BIBLIOTECA/Análise e classificação de Linguagens de Programação Musical.pdf:PDF},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {read},
  url        = {http://periodicos.unespar.edu.br/index.php/vortex/article/view/2614},
}

@Article{Lostanlen2019,
  author     = {Vincent Lostanlen and Joakim Andén and Mathieu Lagrange},
  journal    = {Comptes Rendus Physique},
  title      = {Fourier at the heart of computer music: From harmonic sounds to texture},
  year       = {2019},
  issn       = {1631-0705},
  note       = {Fourier and the science of today / Fourier et la science d’aujourd’hui},
  number     = {5},
  pages      = {461-473},
  volume     = {20},
  abstract   = {Beyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contributions represent some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string. The introduction of Fourier series thus opened new research avenues on the modeling of musical timbre—a topic that was to become of crucial importance in the 1960s with the advent of computer-generated sounds. This article proposes to revisit the scientific legacy of Joseph Fourier through the lens of computer music research. We first discuss how the Fourier series marked a paradigm shift in our understanding of acoustics, supplanting the theory of consonance of harmonics in the Pythagorean monochord. Then, we highlight the utility of Fourier's paradigm via three practical problems in analysis–synthesis: the imitation of musical instruments, frequency transposition, and the generation of audio textures. Interestingly, each of these problems involves a different perspective on time–frequency duality, and stimulates a multidisciplinary interplay between research and creation that is still ongoing.
Résumé
Au-delà de son apport théorique dans le domaine de la conduction thermique, le mémoire de Joseph Fourier sur la Théorie analytique de la chaleur (1822) a révolutionné notre conception des ondes sonores. Ce mémoire affirme que toute fonction de période unitaire se décompose en une série de sinusoïdes, chacune représentant une propriété essentielle du phénomène périodique étudié. En acoustique, cette décomposition révèle les modes de résonance d'une corde vibrante. Ainsi, l'introduction des séries de Fourier a ouvert de nouveaux horizons en matière de modélisation du timbre musical, un sujet qui prendra une importance cruciale à partir des années 1960, avec les débuts de la musique par ordinateur. Cet article propose de thématiser l'œuvre de Joseph Fourier à la lumière de ses implications en recherche musicale. Nous retraçons d'abord le changement de paradigme que les séries de Fourier ont opéré en acoustique, supplantant un mode de pensée fondé sur les consonances du monocorde pythagoricien. Par la suite, nous soulignons l'intérêt du paradigme de Fourier à travers trois problèmes pratiques en analyse-synthèse : l'imitation d'instruments de musique, la transposition fréquentielle, et la génération de textures sonores. Chacun de ses trois problèmes convoque une perspective différente sur la dualité temps–fréquence, et suscite un dialogue multidisciplinaire entre recherche et création qui est toujours d'actualité.},
  doi        = {https://doi.org/10.1016/j.crhy.2019.07.005},
  file       = {:BIBLIOTECA/Fourier at the heart of computer music From harmonic sounds to texture.pdf:PDF},
  keywords   = {Fourier analysis, Computer music, Audio signal processing, Analyse de Fourier, Musique par ordinateur, Traitement du signal audio-numérique},
  priority   = {prio3},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://www.sciencedirect.com/science/article/pii/S1631070519301057},
}

@Article{Voznenko2018,
  author   = {Timofei I. Voznenko and Alexander A. Dyumin and Evgeniya V. Aksenova and Alexander A. Gridnev and Vladislav A. Delov},
  journal  = {Procedia Computer Science},
  title    = {The Experimental Study of ‘Unwanted Music’ Noise Pollution Influence on Command Recognition by Brain-Computer Interface},
  year     = {2018},
  issn     = {1877-0509},
  note     = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
  pages    = {528-533},
  volume   = {123},
  abstract = {Nowadays, the alternative methods of human-computer interactions are in development. These methods can drastically improve usability of cyber-physical systems and devices, as mobile robots, especially for disabled people. Brain-computer interfaces (BCI) are among them. Unfortunately, BCIs aren’t reliable enough to handle critical devices outside lab environments since the quality of command recognition can be influenced by external conditions, as noise pollution that can distract the user of BCI. In this paper, we are presenting the experimental study results of the noise pollution influence in the form of unwanted music on the quality of control through BCI. In general, the obtained results showed the negative impact on the accuracy of control for the most of participants.},
  doi      = {https://doi.org/10.1016/j.procs.2018.01.080},
  file     = {:BIBLIOTECA/The Experimental Study of ‘Unwanted Music’ Noise Pollution Influence on Command Recognition by Brain-Computer Interface.pdf:PDF},
  keywords = {brain-computer interface, music influence, command recognition, robotics},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050918300814},
}

@Article{Barate2017,
  author   = {Adriano Baratè and Luca A. Ludovico and Dario Malchiodi},
  journal  = {Procedia Computer Science},
  title    = {Fostering Computational Thinking in Primary School through a LEGO®-based Music Notation},
  year     = {2017},
  issn     = {1877-0509},
  note     = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
  pages    = {1334-1344},
  volume   = {112},
  abstract = {This paper presents a teaching methodology mixing elements from the domains of music and informatics as a key enabling to expose primary school pupils to basic aspects of computational thinking. This methodology is organized in two phases exploiting LEGO® bricks respectively as a physical tool and as a metaphor in order to let participants discover a simple notation encoding several basic concepts of the classical musical notation. The related activities, grounded on active learning theory, challenge groups of students to solve musical encoding problems of increasing difficulty.},
  doi      = {https://doi.org/10.1016/j.procs.2017.08.018},
  file     = {:BIBLIOTECA/Fostering Computational Thinking in Primary School through a LEGO®-based Music Notation.pdf:PDF},
  keywords = {Music, Education, Visual Programming},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050917313571},
}

@Article{Hsiao2017,
  author   = {Shih-Wen Hsiao and Shih-Kai Chen and Chu-Hsuan Lee},
  journal  = {Information Sciences},
  title    = {Methodology for stage lighting control based on music emotions},
  year     = {2017},
  issn     = {0020-0255},
  pages    = {14-35},
  volume   = {412-413},
  abstract = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music.},
  doi      = {https://doi.org/10.1016/j.ins.2017.05.026},
  file     = {:BIBLIOTECA/Methodology for stage lighting control based on music emotions.pdf:PDF},
  keywords = {Automatic stage-lighting regulation, Music emotion recognition, Lighting color regulation based on music emotions and genre, Support vector regression (SVR), Automatic music segment detection},
  ranking  = {rank5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025517307351},
}

@Article{Silva2020,
  author   = {Angelo Cesar Mendes da Silva and Maurício Archanjo Nunes Coelho and Raul Fonseca Neto},
  journal  = {Expert Systems with Applications},
  title    = {A Music Classification model based on metric learning applied to MP3 audio files},
  year     = {2020},
  issn     = {0957-4174},
  pages    = {113071},
  volume   = {144},
  abstract = {The development of models for learning music similarity from audio media files is an increasingly important task for the entertainment industry. This work proposes a novel music classification model based on metric learning whose main objective is to learn a personalized metric for each customer. The metric learning process considers the learning of a set of parameterized distances employing a structured prediction approach from a set of MP3 audio files containing several music genres according to the users taste. The structured prediction solution aims to maximize the separation margin between genre centroids and to minimize the overall intra-cluster distances. To extract the acoustic information we use the Mel-Frequency Cepstral Coecient (MFCC) and made a dimensionality reduction using Principal Components Analysis (PCA). We attest the model validity performing a set of experiments and comparing the training and testing results with baseline algorithms, such as K-means and Soft Margin Linear Support Vector Machine (SVM). Also, to prove the prediction capacity, we compare our results with two recent works with good prediction results on the GTZAN dataset. Experiments show promising results and encourage the future development of an online version of the learning model to be applied in streaming platforms.},
  doi      = {https://doi.org/10.1016/j.eswa.2019.113071},
  file     = {:BIBLIOTECA/A Music Classification Model based on Metric Learning Applied to MP3 Audio Files.pdf:PDF},
  keywords = {Music similarity, Metric learning, Feature extraction, Mel frequency cepstral coefficient, Principal components analysis},
  ranking  = {rank5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417419307882},
}

@Article{Rocamora2014,
  author   = {Martín Rocamora and Pablo Cancela and Alvaro Pardo},
  journal  = {Pattern Recognition Letters},
  title    = {Query by humming: Automatically building the database from music recordings},
  year     = {2014},
  issn     = {0167-8655},
  pages    = {272-280},
  volume   = {36},
  abstract = {Singing or humming to a music search engine is an appealing multimodal interaction paradigm, particularly for small sized portable devices that are ubiquitous nowadays. The aim of this work is to overcome the main shortcoming of the existing query-by-humming (QBH) systems: their lack of scalability in terms of the difficulty of automatically extending the database of melodies from audio recordings. A method is proposed to extract the singing voice melody from polyphonic music providing the necessary information to index it as an element in the database. The search of a query pattern in the database is carried out combining note sequence matching and pitch time series alignment. A prototype system was developed and experiments are carried out pursuing a fair comparison between manual and automatic expansion of the database. In the light of the obtained performance (85% in the top-10), which is encouraging given the results reported to date, this can be considered a proof of concept that validates the approach.},
  doi      = {https://doi.org/10.1016/j.patrec.2013.04.006},
  file     = {:BIBLIOTECA/Query by humming Automatically building the database from music recordings.pdf:PDF},
  keywords = {Voice based multimodal interfaces, Music information retrieval, Query by humming, Singing voice separation, Melody extraction},
  ranking  = {rank5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167865513001566},
}

@Article{Velankar2015,
  author     = {M.R. Velankar and H.V. Sahasrabuddhe and P.A. Kulkarni},
  journal    = {Procedia Computer Science},
  title      = {Modeling Melody Similarity Using Music Synthesis and Perception},
  year       = {2015},
  issn       = {1877-0509},
  note       = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
  pages      = {728-735},
  volume     = {45},
  abstract   = {Melody similarity in music is a perception of listeners based on cognitive method. Thus, the algorithms should be based on perceptually oriented computational model. We have used computer generated synthesized tune of popular song and its variations to understand similarity notion. We have generated variations of a tune by changing musical scale or relative duration of notes or notes itself and combination of them. The proposed approach to calculate similarity relationship between two tunes will be useful to model the melody similarity notion for various applications such as QBH (Query by humming), music classification and retrieval, music plagiarism etc.},
  doi        = {https://doi.org/10.1016/j.procs.2015.03.141},
  file       = {:BIBLIOTECA/Modeling Melody Similarity Using Music Synthesis and Perception.pdf:PDF},
  keywords   = {music similarity, melody similarity perception, computational model.},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://www.sciencedirect.com/science/article/pii/S1877050915003774},
}

@Article{Guven2012,
  author   = {Erhan Guven and A. Murat Ozbayoglu},
  journal  = {Procedia Computer Science},
  title    = {Note and Timbre Classification by Local Features of Spectrogram},
  year     = {2012},
  issn     = {1877-0509},
  note     = {Complex Adaptive Systems 2012},
  pages    = {182-187},
  volume   = {12},
  abstract = {In recent years, very large scale online music databases containing more than 10 million tracks became prevalent as the fostered availability of streaming and downloading services via the World-Wide Web. The set of access schemes, or Music Information Retrieval (MIR), still poses several and partially solved problems, especially the personalization of the access, such as query by humming, melody, mood, style, genre, instrument, etc. Generally the previous approaches utilized the spectral features of the music track and extracted several high-level features such as pitch, cepstral coefficients, power, and the time-domain features such as onset, tempo, etc. In this work, however, the low-level local features of the spectrogram partitioned by means of the Bark scale are utilized to extract the quantized time-frequency-power features to be used by a Support Vector Machine to classify the notes (melody) and the timbre (instrument) of 128 instruments of General Midi standard. A database of 3-second sound clips of notes C4 to C5 on 7 sound cards using two software synthesizers is constructed and used for experimental note and timbre classification. The preliminary results of 13-category music note and 16-category timbre classifications are promising and their performance scores are surpassing the previously proposed methods.},
  doi      = {https://doi.org/10.1016/j.procs.2012.09.051},
  keywords = {Music information retrieval, music note, timbre, spectrogram features, statistical learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050912006424},
}

@Article{8382228,
  author  = {E. {Zangerle} and C. {Chen} and M. {Tsai} and Y. {Yang}},
  journal = {IEEE Transactions on Affective Computing},
  title   = {Leveraging Affective Hashtags for Ranking Music Recommendations},
  year    = {2018},
  pages   = {1-1},
  doi     = {10.1109/TAFFC.2018.2846596},
}

@Article{9204829,
  author  = {V. {Moscato} and A. {Picariello} and G. {Sperli}},
  journal = {IEEE Intelligent Systems},
  title   = {An emotional recommender system for music},
  year    = {2020},
  pages   = {1-1},
  doi     = {10.1109/MIS.2020.3026000},
}

@Article{9333553,
  author  = {J. R. {Castillo} and M. {Julia Flores}},
  journal = {IEEE Access},
  title   = {Web-based Music Genre Classification for Timeline Song Visualization and Analysis},
  year    = {2021},
  pages   = {1-1},
  doi     = {10.1109/ACCESS.2021.3053864},
  file    = {:BIBLIOTECA/Web-based Music Genre Classification for Timeline Song Visualization and Analysis.pdf:PDF;:BIBLIOTECA/Web music emotion recognition based on higher effective gene expression programming.pdf:PDF},
  ranking = {rank5},
}

@Article{NavarroCaceres2017,
  author    = {Navarro-Cáceres, María and Bajo, Javier and Corchado, Juan Manuel},
  journal   = {Engineering applications of artificial intelligence},
  title     = {Applying social computing to generate sound clouds},
  year      = {2017},
  issn      = {0952-1976},
  pages     = {171--183},
  volume    = {57},
  abstract  = {<p>Human beings make decisions on a daily basis according to their social environment. Social information given by such social contexts provides the basis for inferences, planning and coordination of any activity. Social machines aim to incorporate this concept of social nature and make it possible to design digital systems that make information visible to the users. This paper presents a social machine implemented as a VO where humans and machines collaborate in a creative process to transform a picture into a musical sound cloud. The VO is implemented by a MAS, and defines specialized roles for extracting sounds from the color pixels of the image. As a part of the social machine and in order to demonstrate the viability of the system, the prototype built from this model is evaluated by experts who rate the sounds produced following consonance criteria.</p>},
  keywords  = {Synesthesia ; Social Computing ; Consonance ; Music Generation ; Sound Cloud ; Synesthesia ; Social Computing ; Consonance ; Music Generation ; Sound Cloud ; Applied Sciences ; Computer Science},
  language  = {eng},
  publisher = {Elsevier Ltd},
}

@Article{Geronazzo2019,
  author    = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico and Serafin, Stefania},
  journal   = {Wireless Communications and Mobile Computing},
  title     = {Interactions in Mobile Sound and Music Computing},
  year      = {2019},
  issn      = {1530-8669},
  volume    = {2019},
  keywords  = {Wireless Networks ; Augmented Reality ; Music ; Sound ; Internet of Things ; Musicians & Conductors ; Communication ; Communication Networks ; Musical Performances ; Mobile Computing ; Internet of Things ; Algorithms ; Ubiquitous Computing ; Artificial Intelligence ; Object Recognition ; Case Studies;},
  publisher = {Hindawi},
}

@Article{Alvaro2013,
  author    = {Alvaro, Jesús L and Barros, Beatriz},
  journal   = {Journal of network and computer applications},
  title     = {A new cloud computing architecture for music composition},
  year      = {2013},
  issn      = {1084-8045},
  number    = {1},
  pages     = {429--443},
  volume    = {36},
  abstract  = {<p>This paper presents an original cloud computing architecture for music composition. In this model, music applications are built by making several computer music services work together. Component services are provided by a dedicated layer in the cloud architecture called computer music as a Service (CMaaS). The specialized music services can be integrated into different applications at the same time. These music services provided by the CMaaS layer are implemented in the form of platform images based on templates at the Platform as a Service layer. The images are ready to be loaded into the virtualized infrastructure on demand. As examples of implementation over the proposed cloud architecture, two powerful applications for computer music composition are presented: “Diatonic Composer”, an interactive composer of scores with high-abstraction music elements, and “Csound Meets the Cloud”, an assisted algorithmic composer focused on sound synthesis. The composition model, the involved...},
  keywords  = {Cloud Computing ; Music Composition ; Music Representation ; Computer Architectures ; Algorithmic Composition ; Computer Music ; Cloud Computing ; Music Composition ; Music Representation ; Computer Architectures ; Algorithmic Composition ; Computer Music ; Engineering ; Computer Science},
  language  = {eng},
  publisher = {Elsevier Ltd},
  ranking   = {rank5},
}

@Article{Haworth2015,
  author    = {Haworth, Christopher},
  journal   = {Computer music journal},
  title     = {Sound Synthesis Procedures as Texts: An Ontological Politics in Electroacoustic and Computer Music},
  year      = {2015},
  issn      = {0148-9267},
  number    = {1},
  pages     = {41--58},
  volume    = {39},
  file      = {Artigo:BIBLIOTECA/Sound Synthesis Procedures as Texts Na Ontological Politics in Electroacoustic and Computer Music.pdf:PDF},
  keywords  = {Computer Sound Processing ; Frequency Synthesizers ; Computer Music ; Data Processing ; Music ; Philosophy and Aesthetics ; Music},
  language  = {eng},
  publisher = {The MIT Press},
  url       = {https://muse.jhu.edu/article/577851},
}

@Article{Kendall2014,
  author     = {Kendall, Gary Stephen and Cádiz, Rodrigo},
  journal    = {Computer music journal},
  title      = {Sound Synthesis with Auditory Distortion Products},
  year       = {2014},
  issn       = {0148-9267},
  number     = {4},
  pages      = {5--23},
  volume     = {38},
  file       = {Artigo:BIBLIOTECA/Sound Synthesis with Auditory Distortion Products.pdf:PDF},
  keywords   = {Software Synthesizers ; Computer Sound Processing ; Combination Tones ; Music},
  language   = {eng},
  publisher  = {The MIT Press},
  readstatus = {skimmed},
  url        = {https://muse.jhu.edu/article/564253},
}

@Article{Wang2018,
  author    = {Wang, Jui-Hsien and Qu, Ante and Langlois, Timothy and James, Doug},
  journal   = {ACM transactions on graphics},
  title     = {Toward wave-based sound synthesis for computer animation},
  year      = {2018},
  issn      = {07300301},
  number    = {4},
  pages     = {1--16},
  volume    = {37},
  abstract  = {<p><p>We explore an integrated approach to sound generation that supports a wide variety of physics-based simulation models and computer-animated phenomena. Targeting high-quality offline sound synthesis, we seek to resolve animation-driven sound radiation with near-field scattering and diffraction effects. The core of our approach is a sharp-interface finite-difference time-domain (FDTD) wavesolver, with a series of supporting algorithms to handle rapidly deforming and vibrating embedded interfaces arising in physics-based animation sound. Once the solver rasterizes these interfaces, it must evaluate acceleration boundary conditions (BCs) that involve model-and phenomena-specific computations. We introduce <i>acoustic shaders</i> as a mechanism to abstract away these complexities, and describe a variety of implementations for computer animation: near-rigid objects with ringing and acceleration noise, deformable (finite element) models such as thin shells, bubble-based...},
  keywords  = {Acoustics ; Computer Animation ; Finite-Difference Time-Domain Method ; Sound Synthesis ; Engineering},
  language  = {eng},
  publisher = {ACM},
}

@Article{LuizSchiavoni2018,
  author       = {Luiz Schiavoni, Flávio and Luiz Gonçalves, Luan and Mauro da Silva Sandy, José},
  journal      = {Revista Música Hodie},
  title        = {Mosaicode and the visual programming of web application for music and multimedia},
  year         = {2018},
  month        = {jun.},
  number       = {1},
  pages        = {132 - 146},
  volume       = {18},
  abstractnote = {&lt;div class=&quot;page&quot; title=&quot;Page 1&quot;&gt; &lt;div class=&quot;layoutArea&quot;&gt; &lt;div class=&quot;column&quot;&gt; &lt;p&gt;The development of audio application demands a high knowledge about this application domain, traditional program- ming logic and programming languages. It is possible to use a Visual Programming Language to ease the application development, including experimentations and creative exploration of the language. In this paper we present a Visual Programming Environment to create Web Audio applications, called Mosaicode. Different from other audio creation platforms that use a visual approach, our environment is a source code generator based on code snippets to create complete applications.&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&#38;gt;},
  doi          = {10.5216/mh.v18i1.53577},
  file         = {:BIBLIOTECA/Mosaicode and the visual programming of web application for music and multimedia.pdf:PDF},
  url          = {https://www.revistas.ufg.br/musica/article/view/53577},
}

@Article{article,
  author     = {Pennycook, Bruce},
  journal    = {ACM Comput. Surv.},
  title      = {Computer-Music Interfaces: A Survey},
  year       = {1985},
  month      = {06},
  pages      = {267-289},
  volume     = {17},
  doi        = {10.1145/4468.4470},
  file       = {:BIBLIOTECA/SURVEY/Survey - Computer-Music_Interfaces_A_Survey.pdf:PDF},
  readstatus = {skimmed},
}

@Comment{jabref-meta: databaseType:bibtex;}
