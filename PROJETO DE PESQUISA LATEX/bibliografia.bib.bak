% Encoding: UTF-8

@Article{AraujoeJoseMauroSandyeElderJoseCiriloeFlavioLuizSchiavoni2018,
  author       = {Rodrigo de Araujo e José Mauro Sandy e Elder José Cirilo e Flávio Luiz Schiavoni},
  title        = {Análise e classificação de Linguagens de Programação Musical},
  issn         = {2317–9937},
  number       = {2},
  volume       = {6},
  abstract     = {As linguagens de programação musical datam dos primórdios da computação e sofreram - e ainda sofrem - uma grande influência da evolução e pesquisa na área de Linguagens de Programação.  Esta influência resultou em um ecossistema de linguagens com diferentes paradigmas, mas sob o mesmo domínio, a Computação Musical. Neste artigo, apresentamos as questões históricas da evolução destas linguagens, as suas questões técnicas e de desenvolvimento e também uma análise e avaliação das mesmas levando em consideração a facilidade de uso e critérios como legibilidade, expressividade e facilidade de escrita e também a influência da comunidade no desenvolvimento das mesmas. Por fim, apresentamos uma discussão sobre esta análise e avaliação que pode auxiliar artistas e programadores.},
  date         = {2018},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Análise e classificação de Linguagens de Programação Musical.pdf:PDF;:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Análise e classificação de Linguagens de Programação Musical.tex:LaTeX},
  journaltitle = {Revista Vórtex},
  priority     = {prio1},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {http://periodicos.unespar.edu.br/index.php/vortex/article/view/2614},
}

@Article{Lostanlen2019,
  author       = {Vincent Lostanlen and Joakim Andén and Mathieu Lagrange},
  title        = {Fourier at the heart of computer music: From harmonic sounds to texture},
  issn         = {1631-0705},
  note         = {Fourier and the science of today / Fourier et la science d’aujourd’hui},
  number       = {5},
  pages        = {461-473},
  volume       = {20},
  abstract     = {Beyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contributions represent some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string. The introduction of Fourier series thus opened new research avenues on the modeling of musical timbre—a topic that was to become of crucial importance in the 1960s with the advent of computer-generated sounds. This article proposes to revisit the scientific legacy of Joseph Fourier through the lens of computer music research. We first discuss how the Fourier series marked a paradigm shift in our understanding of acoustics, supplanting the theory of consonance of harmonics in the Pythagorean monochord. Then, we highlight the utility of Fourier's paradigm via three practical problems in analysis–synthesis: the imitation of musical instruments, frequency transposition, and the generation of audio textures. Interestingly, each of these problems involves a different perspective on time–frequency duality, and stimulates a multidisciplinary interplay between research and creation that is still ongoing.
Résumé
Au-delà de son apport théorique dans le domaine de la conduction thermique, le mémoire de Joseph Fourier sur la Théorie analytique de la chaleur (1822) a révolutionné notre conception des ondes sonores. Ce mémoire affirme que toute fonction de période unitaire se décompose en une série de sinusoïdes, chacune représentant une propriété essentielle du phénomène périodique étudié. En acoustique, cette décomposition révèle les modes de résonance d'une corde vibrante. Ainsi, l'introduction des séries de Fourier a ouvert de nouveaux horizons en matière de modélisation du timbre musical, un sujet qui prendra une importance cruciale à partir des années 1960, avec les débuts de la musique par ordinateur. Cet article propose de thématiser l'œuvre de Joseph Fourier à la lumière de ses implications en recherche musicale. Nous retraçons d'abord le changement de paradigme que les séries de Fourier ont opéré en acoustique, supplantant un mode de pensée fondé sur les consonances du monocorde pythagoricien. Par la suite, nous soulignons l'intérêt du paradigme de Fourier à travers trois problèmes pratiques en analyse-synthèse : l'imitation d'instruments de musique, la transposition fréquentielle, et la génération de textures sonores. Chacun de ses trois problèmes convoque une perspective différente sur la dualité temps–fréquence, et suscite un dialogue multidisciplinaire entre recherche et création qui est toujours d'actualité.},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.crhy.2019.07.005},
  file         = {:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Fourier at the heart of computer music From harmonic sounds to texture.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Fourier At The Heart Of Computer Music - From Harmonic Sounds To Texture.tex:TEX},
  journaltitle = {Comptes Rendus Physique},
  keywords     = {Fourier analysis, Computer music, Audio signal processing, Analyse de Fourier, Musique par ordinateur, Traitement du signal audio-numérique},
  priority     = {prio3},
  ranking      = {rank2},
  readstatus   = {read},
  url          = {https://www.sciencedirect.com/science/article/pii/S1631070519301057},
}

@Article{Hsiao2017,
  author       = {Shih-Wen Hsiao and Shih-Kai Chen and Chu-Hsuan Lee},
  title        = {Methodology for stage lighting control based on music emotions},
  issn         = {0020-0255},
  pages        = {14-35},
  volume       = {412-413},
  abstract     = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music.},
  date         = {2017},
  doi          = {https://doi.org/10.1016/j.ins.2017.05.026},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Methodology for stage lighting control based on music emotions.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Methodology for Stage Lighting Control Based on Music Emotions.tex:TEX},
  journaltitle = {Inform Sci},
  keywords     = {Automatic stage-lighting regulation, Music emotion recognition, Lighting color regulation based on music emotions and genre, Support vector regression (SVR), Automatic music segment detection},
  ranking      = {rank5},
  url          = {https://www.sciencedirect.com/science/article/pii/S0020025517307351},
}

@Article{Velankar2015,
  author       = {M.R. Velankar and H.V. Sahasrabuddhe and P.A. Kulkarni},
  date         = {2015},
  journaltitle = {Procedia Computer Science},
  title        = {Modeling Melody Similarity Using Music Synthesis and Perception},
  doi          = {https://doi.org/10.1016/j.procs.2015.03.141},
  issn         = {1877-0509},
  note         = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
  pages        = {728-735},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050915003774},
  volume       = {45},
  abstract     = {Melody similarity in music is a perception of listeners based on cognitive method. Thus, the algorithms should be based on perceptually oriented computational model. We have used computer generated synthesized tune of popular song and its variations to understand similarity notion. We have generated variations of a tune by changing musical scale or relative duration of notes or notes itself and combination of them. The proposed approach to calculate similarity relationship between two tunes will be useful to model the melody similarity notion for various applications such as QBH (Query by humming), music classification and retrieval, music plagiarism etc.},
  file         = {:BIBLIOTECA/Modeling Melody Similarity Using Music Synthesis and Perception.pdf:PDF},
  keywords     = {music similarity, melody similarity perception, computational model.},
  ranking      = {rank4},
  readstatus   = {read},
}

@Article{Zangerle2018,
  author       = {E. {Zangerle} and C. {Chen} and M. {Tsai} and Y. {Yang}},
  title        = {Leveraging Affective Hashtags for Ranking Music Recommendations},
  pages        = {1-1},
  date         = {2018},
  doi          = {10.1109/TAFFC.2018.2846596},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Leveraging Affective Hashtags for Ranking Music Recommendations.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Leveraging Affective Hashtags for Ranking Music Recommendations.tex:TEX},
  journaltitle = {IEEE Transactions on Affective Computing},
  readstatus   = {read},
}

@Article{Castillo2021,
  author       = {J. R. {Castillo} and M. {Julia Flores}},
  title        = {Web-based Music Genre Classification for Timeline Song Visualization and Analysis},
  pages        = {1-1},
  date         = {2021},
  doi          = {10.1109/ACCESS.2021.3053864},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Web-based Music Genre Classification for Timeline Song Visualization and Analysis.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Web-based Music Genre Classification for Timeline Song Visualization and Analysis.tex:TEX},
  journaltitle = {IEEE Access},
  ranking      = {rank5},
}

@Article{Geronazzo2019,
  author       = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico and Serafin, Stefania},
  date         = {2019},
  journaltitle = {Wireless Communications and Mobile Computing},
  title        = {Interactions in Mobile Sound and Music Computing},
  issn         = {1530-8669},
  volume       = {2019},
  keywords     = {Wireless Networks ; Augmented Reality ; Music ; Sound ; Internet of Things ; Musicians & Conductors ; Communication ; Communication Networks ; Musical Performances ; Mobile Computing ; Internet of Things ; Algorithms ; Ubiquitous Computing ; Artificial Intelligence ; Object Recognition ; Case Studies;},
  publisher    = {Hindawi},
}

@Article{Haworth2015,
  author       = {Haworth, Christopher},
  date         = {2015},
  journaltitle = {Computer music journal},
  title        = {Sound Synthesis Procedures as Texts: An Ontological Politics in Electroacoustic and Computer Music},
  issn         = {0148-9267},
  language     = {eng},
  number       = {1},
  pages        = {41--58},
  url          = {https://muse.jhu.edu/article/577851},
  volume       = {39},
  file         = {Artigo:BIBLIOTECA/Sound Synthesis Procedures as Texts Na Ontological Politics in Electroacoustic and Computer Music.pdf:PDF},
  keywords     = {Computer Sound Processing ; Frequency Synthesizers ; Computer Music ; Data Processing ; Music ; Philosophy and Aesthetics ; Music},
  publisher    = {The MIT Press},
}

@Article{Kendall2014,
  author       = {Kendall, Gary Stephen and Cádiz, Rodrigo},
  date         = {2014},
  journaltitle = {Computer music journal},
  title        = {Sound Synthesis with Auditory Distortion Products},
  issn         = {0148-9267},
  language     = {eng},
  number       = {4},
  pages        = {5--23},
  url          = {https://muse.jhu.edu/article/564253},
  volume       = {38},
  file         = {Artigo:BIBLIOTECA/Sound Synthesis with Auditory Distortion Products.pdf:PDF},
  keywords     = {Software Synthesizers ; Computer Sound Processing ; Combination Tones ; Music},
  publisher    = {The MIT Press},
  readstatus   = {skimmed},
}

@Article{LuizSchiavoni2018,
  author       = {Luiz Schiavoni, Flávio and Luiz Gonçalves, Luan and Mauro da Silva Sandy, José},
  date         = {2018-06},
  journaltitle = {Revista Música Hodie},
  title        = {Mosaicode and the visual programming of web application for music and multimedia},
  doi          = {10.5216/mh.v18i1.53577},
  number       = {1},
  pages        = {132 - 146},
  url          = {https://www.revistas.ufg.br/musica/article/view/53577},
  volume       = {18},
  abstractnote = {&lt;div class=&quot;page&quot; title=&quot;Page 1&quot;&gt; &lt;div class=&quot;layoutArea&quot;&gt; &lt;div class=&quot;column&quot;&gt; &lt;p&gt;The development of audio application demands a high knowledge about this application domain, traditional program- ming logic and programming languages. It is possible to use a Visual Programming Language to ease the application development, including experimentations and creative exploration of the language. In this paper we present a Visual Programming Environment to create Web Audio applications, called Mosaicode. Different from other audio creation platforms that use a visual approach, our environment is a source code generator based on code snippets to create complete applications.&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&#38;gt;},
  file         = {:BIBLIOTECA/Mosaicode and the visual programming of web application for music and multimedia.pdf:PDF},
}

@Article{Pennycook1985,
  author       = {Pennycook, Bruce},
  title        = {Computer-Music Interfaces: A Survey},
  pages        = {267-289},
  volume       = {17},
  date         = {1985-06},
  doi          = {10.1145/4468.4470},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/SURVEY/Computer Music Interfaces A Survey.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Computer-Music Interfaces - A Survey.tex:TEX},
  journaltitle = {ACM Comput. Surv.},
  readstatus   = {skimmed},
}

@Article{8327886,
  author  = {R. {Panda} and R. {Malheiro} and R. P. {Paiva}},
  journal = {IEEE Transactions on Affective Computing},
  title   = {Novel Audio Features for Music Emotion Recognition},
  year    = {2020},
  number  = {4},
  pages   = {614-626},
  volume  = {11},
  doi     = {10.1109/TAFFC.2018.2820691},
  file    = {:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Novel Audio Features for Music Emotion Recognition.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Novel Audio Features for Music Emotion Recognition.tex:TEX},
}

@Article{Rumiantcev2020,
  author    = {Mikhail Rumiantcev and Oleksiy Khriyenko},
  journal   = {Proceedings of the XXth Conference of Open Innovations Association FRUCT},
  title     = {Emotion Based Music Recommendation System},
  year      = {2020},
  issn      = {2305-7254},
  number    = {2},
  pages     = {639--645},
  volume    = {26},
  abstract  = {<p>Nowadays, music platforms provide easy access to large amounts of music. They are working continuously to improve music organization and search management thereby addressing the problem of choice and simplify exploring new music pieces. Recommendation systems gain more and more popularity and help people to select appropriate music for all occasions. However, there is still a gap in personalization and emotions driven recommendations. Music has great influence on humans and is widely used for relaxing, mood regulation, destruction from stress and diseases, to maintain mental and physical work. There is a wide range of clinical settings and practices in music therapy for the wellbeing support. This paper will present design of the personalized music recommendation system, driven by listener feelings, emotions and activity contexts. With combination of artificial intelligent technologies and generalized music therapy approaches, recommendation system is targeted to help people with...},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Emotion Based Music Recommendation System.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Emotion Based Music Recommendation System .tex:TEX},
  keywords  = {Artificial Intelligence ; Machine Learning ; Music Curation ; Recommendation System},
  language  = {eng},
  publisher = {FRUCT},
  url       = {https://doaj.org/article/21bcc31afa7a4adea0c95fc4e82f68db},
}

@Article{Zhang2013,
  author   = {Kejun Zhang and Shouqian Sun},
  journal  = {Neurocomputing},
  title    = {Web music emotion recognition based on higher effective gene expression programming},
  year     = {2013},
  issn     = {0925-2312},
  note     = {Learning for Scalable Multimedia Representation},
  pages    = {100-106},
  volume   = {105},
  abstract = {In the study, we present a higher effective algorithm, called revised gene expression programming (RGEP), to construct the model for music emotion recognition. Our main contributions are as follows: firstly, we describe the basic mechanisms of music emotion recognition and introduce gene expression programming (GEP) to deal with the model construction for music emotion recognition. Secondly, we present RGEP based on backward-chaining evolutionary algorithm and use GEP, RGEP, and SVM to construct the models for music emotion recognition separately, the results show that the models obtained by SVM, GEP, and RGEP are satisfactory and well confirm the experimental values. Finally, we report the comparison of these models, and we find that the model obtained by RGEP outperforms classification accuracy of the model by GEP and takes almost 15% less processing time of GEP and even half processing time of SVM, which offers a new efficient way for solving music emotion recognition problems; moreover, because processing time is essential for the problem of large scale music information retrieval, therefore, RGEP might prompt the development of the music information retrieval technology.},
  doi      = {https://doi.org/10.1016/j.neucom.2012.06.041},
  file     = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Web music emotion recognition based on higher effective gene expression programming.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Web Music Emotion Recognition Based on Higher Effective Gene Expression Programming.tex:TEX},
  keywords = {Music emotion recognition, Evolutionary algorithm, Gene expression programming, Support vector machine, Music information retrieval},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231212007035},
}

@InProceedings{8405505,
  author    = {S. {Sharma} and P. {Fulzele} and I. {Sreedevi}},
  booktitle = {2018 IEEE Symposium on Computer Applications Industrial Electronics (ISCAIE)},
  title     = {Novel hybrid model for music genre classification based on support vector machine},
  year      = {2018},
  pages     = {395-400},
  doi       = {10.1109/ISCAIE.2018.8405505},
  file      = {Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Novel hybrid model for music genre classification based on support vector machine.tex:TEX;Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Novel hybrid model for music genre classification based on support vector machine.pdf:PDF},
}

@Article{Lee2016,
  author    = {Lee, Yuan-Shan and Chiang, Yen-Lin and Lin, Pei-Rung and Lin, Chang-Hung and Tai, Tzu-Chiang},
  journal   = {APSIPA Transactions on Signal and Information Processing},
  title     = {Robust and efficient content-based music retrieval system},
  year      = {2016},
  issn      = {20487703},
  volume    = {5},
  abstract  = {This work proposes a query-by-singing (QBS) content-based music retrieval (CBMR) system that uses Approximate Karbunen-Loeve transform for noise reduction. The proposed QBS-CBMR system uses a music clip as a search key. First, a 51-dimensional matrix containing 39-Mel-frequency cepstral coefficients (MFCCs) features and 12-Chroma features are extracted from an input music clip. Next, adapted symbolic aggregate approximation (adapted SAX) is used to transform each dimension of features into a symbolic sequence. Each symbolic sequence corresponding to each dimension of MFCCs is then converted into a structure called advanced fast pattern index (AFPI) tree. The similarity between the query music clip and the songs in the database is evaluated by calculating a partial score for each AFPI tree. The final score is obtained by calculating the weighted sum of all partial scores, where the weighting of each partial score is determined by its entropy. Experimental results show that the proposed music...},
  address   = {Cambridge},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Robust and efficient content-based music retrieval system.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Robust and efficient content-based music retrieval system.tex:TEX},
  keywords  = {Feature Extraction ; Accuracy ; Entropy ; Weighting ; Mathematical Analysis ; Computer Science ; Noise Reduction ; Musical Performances ; Signal Processing ; Neural Networks ; Singing ; Retrieval ; Noise ; Acoustics ; Queries ; Speech ; Efficiency ; Databases ; Query-By-Singing ; Music Retrieval},
  language  = {eng},
  publisher = {Cambridge University Press},
  url       = {http://search.proquest.com/docview/1966379595/},
}

@Article{Zampoglou2014,
  author     = {Zampoglou, Markos and Malamos, Athanasios G},
  journal    = {The new review of hypermedia and multimedia},
  title      = {Music information retrieval in compressed audio files: a survey},
  year       = {2014},
  issn       = {1361-4568},
  number     = {3},
  pages      = {189--206},
  volume     = {20},
  abstract   = {<p>In this paper, we present an organized survey of the existing literature on music information retrieval systems in which descriptor features are extracted directly from the compressed audio files, without prior decompression to pulse-code modulation format. Avoiding the decompression step and utilizing the readily available compressed-domain information can significantly lighten the computational cost of a music information retrieval system, allowing application to large-scale music databases. We identify a number of systems relying on compressed-domain information and form a systematic classification of the features they extract, the retrieval tasks they tackle and the degree in which they achieve an actual increase in the overall speed-as well as any resulting loss in accuracy. Finally, we discuss recent developments in the field, and the potential research directions they open toward ultra-fast, scalable systems.</p>},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Music information retrieval in compressed audio files a survey.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Music information retrieval in compressed audio files a survey.tex:TEX},
  keywords   = {Music Information Retrieval ; Audio Compression ; Mpeg 7 ; Computer Science},
  language   = {eng},
  publisher  = {Taylor & Francis},
  readstatus = {read},
  url        = {http://www.tandfonline.com/doi/abs/10.1080/13614568.2014.889223},
}

@Article{Choi2018,
  author    = {Choi, Keunwoo and Fazekas, György and Cho, Kyunghyun and Sandler, Mark},
  journal   = {arXiv.org},
  title     = {A Tutorial on Deep Learning for Music Information Retrieval},
  year      = {2018},
  issn      = {2331-8422},
  abstract  = {Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.},
  address   = {Ithaca},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/A Tutorial on Deep Learning for Music Information Retrieval.pdf:PDF;FICHAMENTO:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento A Tutorial on Deep Learning for Music Information Retrieval.tex:TEX},
  keywords  = {Information Retrieval ; Music ; Computer Vision ; Machine Learning ; Information Retrieval ; Domains ; Computer Vision and Pattern Recognition ; Sound},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  url       = {http://search.proquest.com/docview/2072233560/},
}

@Comment{jabref-meta: databaseType:bibtex;}
