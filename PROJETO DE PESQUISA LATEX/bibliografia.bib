% Encoding: UTF-8

@Article{AraujoeJoseMauroSandyeElderJoseCiriloeFlavioLuizSchiavoni2018,
  author       = {Rodrigo de Araujo e José Mauro Sandy e Elder José Cirilo e Flávio Luiz Schiavoni},
  date         = {2018},
  journaltitle = {Revista Vórtex},
  title        = {Análise e classificação de Linguagens de Programação Musical},
  issn         = {2317–9937},
  number       = {2},
  url          = {http://periodicos.unespar.edu.br/index.php/vortex/article/view/2614},
  volume       = {6},
  abstract     = {As linguagens de programação musical datam dos primórdios da computação e sofreram - e ainda sofrem - uma grande influência da evolução e pesquisa na área de Linguagens de Programação.  Esta influência resultou em um ecossistema de linguagens com diferentes paradigmas, mas sob o mesmo domínio, a Computação Musical. Neste artigo, apresentamos as questões históricas da evolução destas linguagens, as suas questões técnicas e de desenvolvimento e também uma análise e avaliação das mesmas levando em consideração a facilidade de uso e critérios como legibilidade, expressividade e facilidade de escrita e também a influência da comunidade no desenvolvimento das mesmas. Por fim, apresentamos uma discussão sobre esta análise e avaliação que pode auxiliar artistas e programadores.},
  file         = {:BIBLIOTECA/Análise e classificação de Linguagens de Programação Musical.pdf:PDF},
  priority     = {prio1},
  ranking      = {rank5},
  readstatus   = {read},
}

@Article{Lostanlen2019,
  author       = {Vincent Lostanlen and Joakim Andén and Mathieu Lagrange},
  date         = {2019},
  journaltitle = {Comptes Rendus Physique},
  title        = {Fourier at the heart of computer music: From harmonic sounds to texture},
  doi          = {https://doi.org/10.1016/j.crhy.2019.07.005},
  issn         = {1631-0705},
  note         = {Fourier and the science of today / Fourier et la science d’aujourd’hui},
  number       = {5},
  pages        = {461-473},
  url          = {https://www.sciencedirect.com/science/article/pii/S1631070519301057},
  volume       = {20},
  abstract     = {Beyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contributions represent some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string. The introduction of Fourier series thus opened new research avenues on the modeling of musical timbre—a topic that was to become of crucial importance in the 1960s with the advent of computer-generated sounds. This article proposes to revisit the scientific legacy of Joseph Fourier through the lens of computer music research. We first discuss how the Fourier series marked a paradigm shift in our understanding of acoustics, supplanting the theory of consonance of harmonics in the Pythagorean monochord. Then, we highlight the utility of Fourier's paradigm via three practical problems in analysis–synthesis: the imitation of musical instruments, frequency transposition, and the generation of audio textures. Interestingly, each of these problems involves a different perspective on time–frequency duality, and stimulates a multidisciplinary interplay between research and creation that is still ongoing.
Résumé
Au-delà de son apport théorique dans le domaine de la conduction thermique, le mémoire de Joseph Fourier sur la Théorie analytique de la chaleur (1822) a révolutionné notre conception des ondes sonores. Ce mémoire affirme que toute fonction de période unitaire se décompose en une série de sinusoïdes, chacune représentant une propriété essentielle du phénomène périodique étudié. En acoustique, cette décomposition révèle les modes de résonance d'une corde vibrante. Ainsi, l'introduction des séries de Fourier a ouvert de nouveaux horizons en matière de modélisation du timbre musical, un sujet qui prendra une importance cruciale à partir des années 1960, avec les débuts de la musique par ordinateur. Cet article propose de thématiser l'œuvre de Joseph Fourier à la lumière de ses implications en recherche musicale. Nous retraçons d'abord le changement de paradigme que les séries de Fourier ont opéré en acoustique, supplantant un mode de pensée fondé sur les consonances du monocorde pythagoricien. Par la suite, nous soulignons l'intérêt du paradigme de Fourier à travers trois problèmes pratiques en analyse-synthèse : l'imitation d'instruments de musique, la transposition fréquentielle, et la génération de textures sonores. Chacun de ses trois problèmes convoque une perspective différente sur la dualité temps–fréquence, et suscite un dialogue multidisciplinaire entre recherche et création qui est toujours d'actualité.},
  file         = {:BIBLIOTECA/Fourier at the heart of computer music From harmonic sounds to texture.pdf:PDF},
  keywords     = {Fourier analysis, Computer music, Audio signal processing, Analyse de Fourier, Musique par ordinateur, Traitement du signal audio-numérique},
  priority     = {prio3},
  ranking      = {rank2},
  readstatus   = {read},
}

@Article{Voznenko2018,
  author       = {Timofei I. Voznenko and Alexander A. Dyumin and Evgeniya V. Aksenova and Alexander A. Gridnev and Vladislav A. Delov},
  date         = {2018},
  journaltitle = {Procedia Computer Science},
  title        = {The Experimental Study of ‘Unwanted Music’ Noise Pollution Influence on Command Recognition by Brain-Computer Interface},
  doi          = {https://doi.org/10.1016/j.procs.2018.01.080},
  issn         = {1877-0509},
  note         = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
  pages        = {528-533},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050918300814},
  volume       = {123},
  abstract     = {Nowadays, the alternative methods of human-computer interactions are in development. These methods can drastically improve usability of cyber-physical systems and devices, as mobile robots, especially for disabled people. Brain-computer interfaces (BCI) are among them. Unfortunately, BCIs aren’t reliable enough to handle critical devices outside lab environments since the quality of command recognition can be influenced by external conditions, as noise pollution that can distract the user of BCI. In this paper, we are presenting the experimental study results of the noise pollution influence in the form of unwanted music on the quality of control through BCI. In general, the obtained results showed the negative impact on the accuracy of control for the most of participants.},
  file         = {:BIBLIOTECA/The Experimental Study of ‘Unwanted Music’ Noise Pollution Influence on Command Recognition by Brain-Computer Interface.pdf:PDF},
  keywords     = {brain-computer interface, music influence, command recognition, robotics},
}

@Article{Hsiao2017,
  author       = {Shih-Wen Hsiao and Shih-Kai Chen and Chu-Hsuan Lee},
  date         = {2017},
  journaltitle = {Information Sciences},
  title        = {Methodology for stage lighting control based on music emotions},
  doi          = {https://doi.org/10.1016/j.ins.2017.05.026},
  issn         = {0020-0255},
  pages        = {14-35},
  url          = {https://www.sciencedirect.com/science/article/pii/S0020025517307351},
  volume       = {412-413},
  abstract     = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music.},
  file         = {:BIBLIOTECA/Methodology for stage lighting control based on music emotions.pdf:PDF},
  keywords     = {Automatic stage-lighting regulation, Music emotion recognition, Lighting color regulation based on music emotions and genre, Support vector regression (SVR), Automatic music segment detection},
  ranking      = {rank5},
}

@Article{Silva2020,
  author       = {Angelo Cesar Mendes da Silva and Maurício Archanjo Nunes Coelho and Raul Fonseca Neto},
  date         = {2020},
  journaltitle = {Expert Systems with Applications},
  title        = {A Music Classification model based on metric learning applied to MP3 audio files},
  doi          = {https://doi.org/10.1016/j.eswa.2019.113071},
  issn         = {0957-4174},
  pages        = {113071},
  url          = {https://www.sciencedirect.com/science/article/pii/S0957417419307882},
  volume       = {144},
  abstract     = {The development of models for learning music similarity from audio media files is an increasingly important task for the entertainment industry. This work proposes a novel music classification model based on metric learning whose main objective is to learn a personalized metric for each customer. The metric learning process considers the learning of a set of parameterized distances employing a structured prediction approach from a set of MP3 audio files containing several music genres according to the users taste. The structured prediction solution aims to maximize the separation margin between genre centroids and to minimize the overall intra-cluster distances. To extract the acoustic information we use the Mel-Frequency Cepstral Coecient (MFCC) and made a dimensionality reduction using Principal Components Analysis (PCA). We attest the model validity performing a set of experiments and comparing the training and testing results with baseline algorithms, such as K-means and Soft Margin Linear Support Vector Machine (SVM). Also, to prove the prediction capacity, we compare our results with two recent works with good prediction results on the GTZAN dataset. Experiments show promising results and encourage the future development of an online version of the learning model to be applied in streaming platforms.},
  file         = {:BIBLIOTECA/A Music Classification Model based on Metric Learning Applied to MP3 Audio Files.pdf:PDF},
  keywords     = {Music similarity, Metric learning, Feature extraction, Mel frequency cepstral coefficient, Principal components analysis},
  ranking      = {rank5},
}

@Article{Velankar2015,
  author       = {M.R. Velankar and H.V. Sahasrabuddhe and P.A. Kulkarni},
  date         = {2015},
  journaltitle = {Procedia Computer Science},
  title        = {Modeling Melody Similarity Using Music Synthesis and Perception},
  doi          = {https://doi.org/10.1016/j.procs.2015.03.141},
  issn         = {1877-0509},
  note         = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
  pages        = {728-735},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050915003774},
  volume       = {45},
  abstract     = {Melody similarity in music is a perception of listeners based on cognitive method. Thus, the algorithms should be based on perceptually oriented computational model. We have used computer generated synthesized tune of popular song and its variations to understand similarity notion. We have generated variations of a tune by changing musical scale or relative duration of notes or notes itself and combination of them. The proposed approach to calculate similarity relationship between two tunes will be useful to model the melody similarity notion for various applications such as QBH (Query by humming), music classification and retrieval, music plagiarism etc.},
  file         = {:BIBLIOTECA/Modeling Melody Similarity Using Music Synthesis and Perception.pdf:PDF},
  keywords     = {music similarity, melody similarity perception, computational model.},
  ranking      = {rank4},
  readstatus   = {read},
}

@Article{Guven2012,
  author       = {Erhan Guven and A. Murat Ozbayoglu},
  date         = {2012},
  journaltitle = {Procedia Computer Science},
  title        = {Note and Timbre Classification by Local Features of Spectrogram},
  doi          = {https://doi.org/10.1016/j.procs.2012.09.051},
  issn         = {1877-0509},
  note         = {Complex Adaptive Systems 2012},
  pages        = {182-187},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050912006424},
  volume       = {12},
  abstract     = {In recent years, very large scale online music databases containing more than 10 million tracks became prevalent as the fostered availability of streaming and downloading services via the World-Wide Web. The set of access schemes, or Music Information Retrieval (MIR), still poses several and partially solved problems, especially the personalization of the access, such as query by humming, melody, mood, style, genre, instrument, etc. Generally the previous approaches utilized the spectral features of the music track and extracted several high-level features such as pitch, cepstral coefficients, power, and the time-domain features such as onset, tempo, etc. In this work, however, the low-level local features of the spectrogram partitioned by means of the Bark scale are utilized to extract the quantized time-frequency-power features to be used by a Support Vector Machine to classify the notes (melody) and the timbre (instrument) of 128 instruments of General Midi standard. A database of 3-second sound clips of notes C4 to C5 on 7 sound cards using two software synthesizers is constructed and used for experimental note and timbre classification. The preliminary results of 13-category music note and 16-category timbre classifications are promising and their performance scores are surpassing the previously proposed methods.},
  keywords     = {Music information retrieval, music note, timbre, spectrogram features, statistical learning},
}

@Article{Zangerle2018,
  author       = {E. {Zangerle} and C. {Chen} and M. {Tsai} and Y. {Yang}},
  date         = {2018},
  journaltitle = {IEEE Transactions on Affective Computing},
  title        = {Leveraging Affective Hashtags for Ranking Music Recommendations},
  doi          = {10.1109/TAFFC.2018.2846596},
  pages        = {1-1},
}

@Article{Moscato2020,
  author       = {V. {Moscato} and A. {Picariello} and G. {Sperli}},
  date         = {2020},
  journaltitle = {IEEE Intelligent Systems},
  title        = {An emotional recommender system for music},
  doi          = {10.1109/MIS.2020.3026000},
  pages        = {1-1},
  file         = {:BIBLIOTECA/Emotion Based Music Recommendation System.pdf:PDF},
  readstatus   = {read},
}

@Article{Castillo2021,
  author       = {J. R. {Castillo} and M. {Julia Flores}},
  date         = {2021},
  journaltitle = {IEEE Access},
  title        = {Web-based Music Genre Classification for Timeline Song Visualization and Analysis},
  doi          = {10.1109/ACCESS.2021.3053864},
  pages        = {1-1},
  file         = {:BIBLIOTECA/Web-based Music Genre Classification for Timeline Song Visualization and Analysis.pdf:PDF;:BIBLIOTECA/Web music emotion recognition based on higher effective gene expression programming.pdf:PDF},
  ranking      = {rank5},
}

@Article{NavarroCaceres2017,
  author       = {Navarro-Cáceres, María and Bajo, Javier and Corchado, Juan Manuel},
  date         = {2017},
  journaltitle = {Engineering applications of artificial intelligence},
  title        = {Applying social computing to generate sound clouds},
  issn         = {0952-1976},
  language     = {eng},
  pages        = {171--183},
  volume       = {57},
  abstract     = {<p>Human beings make decisions on a daily basis according to their social environment. Social information given by such social contexts provides the basis for inferences, planning and coordination of any activity. Social machines aim to incorporate this concept of social nature and make it possible to design digital systems that make information visible to the users. This paper presents a social machine implemented as a VO where humans and machines collaborate in a creative process to transform a picture into a musical sound cloud. The VO is implemented by a MAS, and defines specialized roles for extracting sounds from the color pixels of the image. As a part of the social machine and in order to demonstrate the viability of the system, the prototype built from this model is evaluated by experts who rate the sounds produced following consonance criteria.</p>},
  keywords     = {Synesthesia ; Social Computing ; Consonance ; Music Generation ; Sound Cloud ; Synesthesia ; Social Computing ; Consonance ; Music Generation ; Sound Cloud ; Applied Sciences ; Computer Science},
  publisher    = {Elsevier Ltd},
}

@Article{Geronazzo2019,
  author       = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico and Serafin, Stefania},
  date         = {2019},
  journaltitle = {Wireless Communications and Mobile Computing},
  title        = {Interactions in Mobile Sound and Music Computing},
  issn         = {1530-8669},
  volume       = {2019},
  keywords     = {Wireless Networks ; Augmented Reality ; Music ; Sound ; Internet of Things ; Musicians & Conductors ; Communication ; Communication Networks ; Musical Performances ; Mobile Computing ; Internet of Things ; Algorithms ; Ubiquitous Computing ; Artificial Intelligence ; Object Recognition ; Case Studies;},
  publisher    = {Hindawi},
}

@Article{Alvaro2013,
  author       = {Alvaro, Jesús L and Barros, Beatriz},
  date         = {2013},
  journaltitle = {Journal of network and computer applications},
  title        = {A new cloud computing architecture for music composition},
  issn         = {1084-8045},
  language     = {eng},
  number       = {1},
  pages        = {429--443},
  volume       = {36},
  abstract     = {<p>This paper presents an original cloud computing architecture for music composition. In this model, music applications are built by making several computer music services work together. Component services are provided by a dedicated layer in the cloud architecture called computer music as a Service (CMaaS). The specialized music services can be integrated into different applications at the same time. These music services provided by the CMaaS layer are implemented in the form of platform images based on templates at the Platform as a Service layer. The images are ready to be loaded into the virtualized infrastructure on demand. As examples of implementation over the proposed cloud architecture, two powerful applications for computer music composition are presented: “Diatonic Composer”, an interactive composer of scores with high-abstraction music elements, and “Csound Meets the Cloud”, an assisted algorithmic composer focused on sound synthesis. The composition model, the involved...},
  keywords     = {Cloud Computing ; Music Composition ; Music Representation ; Computer Architectures ; Algorithmic Composition ; Computer Music ; Cloud Computing ; Music Composition ; Music Representation ; Computer Architectures ; Algorithmic Composition ; Computer Music ; Engineering ; Computer Science},
  publisher    = {Elsevier Ltd},
  ranking      = {rank5},
}

@Article{Haworth2015,
  author       = {Haworth, Christopher},
  date         = {2015},
  journaltitle = {Computer music journal},
  title        = {Sound Synthesis Procedures as Texts: An Ontological Politics in Electroacoustic and Computer Music},
  issn         = {0148-9267},
  language     = {eng},
  number       = {1},
  pages        = {41--58},
  url          = {https://muse.jhu.edu/article/577851},
  volume       = {39},
  file         = {Artigo:BIBLIOTECA/Sound Synthesis Procedures as Texts Na Ontological Politics in Electroacoustic and Computer Music.pdf:PDF},
  keywords     = {Computer Sound Processing ; Frequency Synthesizers ; Computer Music ; Data Processing ; Music ; Philosophy and Aesthetics ; Music},
  publisher    = {The MIT Press},
}

@Article{Kendall2014,
  author       = {Kendall, Gary Stephen and Cádiz, Rodrigo},
  date         = {2014},
  journaltitle = {Computer music journal},
  title        = {Sound Synthesis with Auditory Distortion Products},
  issn         = {0148-9267},
  language     = {eng},
  number       = {4},
  pages        = {5--23},
  url          = {https://muse.jhu.edu/article/564253},
  volume       = {38},
  file         = {Artigo:BIBLIOTECA/Sound Synthesis with Auditory Distortion Products.pdf:PDF},
  keywords     = {Software Synthesizers ; Computer Sound Processing ; Combination Tones ; Music},
  publisher    = {The MIT Press},
  readstatus   = {skimmed},
}

@Article{LuizSchiavoni2018,
  author       = {Luiz Schiavoni, Flávio and Luiz Gonçalves, Luan and Mauro da Silva Sandy, José},
  date         = {2018-06},
  journaltitle = {Revista Música Hodie},
  title        = {Mosaicode and the visual programming of web application for music and multimedia},
  doi          = {10.5216/mh.v18i1.53577},
  number       = {1},
  pages        = {132 - 146},
  url          = {https://www.revistas.ufg.br/musica/article/view/53577},
  volume       = {18},
  abstractnote = {&lt;div class=&quot;page&quot; title=&quot;Page 1&quot;&gt; &lt;div class=&quot;layoutArea&quot;&gt; &lt;div class=&quot;column&quot;&gt; &lt;p&gt;The development of audio application demands a high knowledge about this application domain, traditional program- ming logic and programming languages. It is possible to use a Visual Programming Language to ease the application development, including experimentations and creative exploration of the language. In this paper we present a Visual Programming Environment to create Web Audio applications, called Mosaicode. Different from other audio creation platforms that use a visual approach, our environment is a source code generator based on code snippets to create complete applications.&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&#38;gt;},
  file         = {:BIBLIOTECA/Mosaicode and the visual programming of web application for music and multimedia.pdf:PDF},
}

@Article{Pennycook1985,
  author       = {Pennycook, Bruce},
  date         = {1985-06},
  journaltitle = {ACM Comput. Surv.},
  title        = {Computer-Music Interfaces: A Survey},
  doi          = {10.1145/4468.4470},
  pages        = {267-289},
  volume       = {17},
  file         = {:BIBLIOTECA/SURVEY/Survey - Computer-Music_Interfaces_A_Survey.pdf:PDF},
  readstatus   = {skimmed},
}

@InProceedings{Lin2016,
  author    = {C. {Lin} and M. {Liu} and W. {Hsiung} and J. {Jhang}},
  booktitle = {2016 International Conference on Machine Learning and Cybernetics (ICMLC)},
  date      = {2016},
  title     = {Music emotion recognition based on two-level support vector classification},
  doi       = {10.1109/ICMLC.2016.7860930},
  pages     = {375-389},
  volume    = {1},
  file      = {:BIBLIOTECA/Music emotion recognition based on two-level support vector classification.pdf:PDF},
}

@Comment{jabref-meta: databaseType:bibtex;}
