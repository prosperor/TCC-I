% Encoding: UTF-8

@Article{AraujoeJoseMauroSandyeElderJoseCiriloeFlavioLuizSchiavoni2018,
  author       = {Rodrigo de Araujo e José Mauro Sandy e Elder José Cirilo e Flávio Luiz Schiavoni},
  title        = {Análise e classificação de Linguagens de Programação Musical},
  issn         = {2317–9937},
  number       = {2},
  volume       = {6},
  abstract     = {As linguagens de programação musical datam dos primórdios da computação e sofreram - e ainda sofrem - uma grande influência da evolução e pesquisa na área de Linguagens de Programação.  Esta influência resultou em um ecossistema de linguagens com diferentes paradigmas, mas sob o mesmo domínio, a Computação Musical. Neste artigo, apresentamos as questões históricas da evolução destas linguagens, as suas questões técnicas e de desenvolvimento e também uma análise e avaliação das mesmas levando em consideração a facilidade de uso e critérios como legibilidade, expressividade e facilidade de escrita e também a influência da comunidade no desenvolvimento das mesmas. Por fim, apresentamos uma discussão sobre esta análise e avaliação que pode auxiliar artistas e programadores.},
  date         = {2018},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Análise e classificação de Linguagens de Programação Musical.pdf:PDF;:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Análise e classificação de Linguagens de Programação Musical.tex:LaTeX},
  journaltitle = {Revista Vórtex},
  priority     = {prio1},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {http://periodicos.unespar.edu.br/index.php/vortex/article/view/2614},
}

@Article{Lostanlen2019,
  author       = {Vincent Lostanlen and Joakim Andén and Mathieu Lagrange},
  title        = {Fourier at the heart of computer music: From harmonic sounds to texture},
  issn         = {1631-0705},
  note         = {Fourier and the science of today / Fourier et la science d’aujourd’hui},
  number       = {5},
  pages        = {461-473},
  volume       = {20},
  abstract     = {Beyond the scope of thermal conduction, Joseph Fourier's treatise on the Analytical Theory of Heat (1822) profoundly altered our understanding of acoustic waves. It posits that any function of unit period can be decomposed into a sum of sinusoids, whose respective contributions represent some essential property of the underlying periodic phenomenon. In acoustics, such a decomposition reveals the resonant modes of a freely vibrating string. The introduction of Fourier series thus opened new research avenues on the modeling of musical timbre—a topic that was to become of crucial importance in the 1960s with the advent of computer-generated sounds. This article proposes to revisit the scientific legacy of Joseph Fourier through the lens of computer music research. We first discuss how the Fourier series marked a paradigm shift in our understanding of acoustics, supplanting the theory of consonance of harmonics in the Pythagorean monochord. Then, we highlight the utility of Fourier's paradigm via three practical problems in analysis–synthesis: the imitation of musical instruments, frequency transposition, and the generation of audio textures. Interestingly, each of these problems involves a different perspective on time–frequency duality, and stimulates a multidisciplinary interplay between research and creation that is still ongoing.
Résumé
Au-delà de son apport théorique dans le domaine de la conduction thermique, le mémoire de Joseph Fourier sur la Théorie analytique de la chaleur (1822) a révolutionné notre conception des ondes sonores. Ce mémoire affirme que toute fonction de période unitaire se décompose en une série de sinusoïdes, chacune représentant une propriété essentielle du phénomène périodique étudié. En acoustique, cette décomposition révèle les modes de résonance d'une corde vibrante. Ainsi, l'introduction des séries de Fourier a ouvert de nouveaux horizons en matière de modélisation du timbre musical, un sujet qui prendra une importance cruciale à partir des années 1960, avec les débuts de la musique par ordinateur. Cet article propose de thématiser l'œuvre de Joseph Fourier à la lumière de ses implications en recherche musicale. Nous retraçons d'abord le changement de paradigme que les séries de Fourier ont opéré en acoustique, supplantant un mode de pensée fondé sur les consonances du monocorde pythagoricien. Par la suite, nous soulignons l'intérêt du paradigme de Fourier à travers trois problèmes pratiques en analyse-synthèse : l'imitation d'instruments de musique, la transposition fréquentielle, et la génération de textures sonores. Chacun de ses trois problèmes convoque une perspective différente sur la dualité temps–fréquence, et suscite un dialogue multidisciplinaire entre recherche et création qui est toujours d'actualité.},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.crhy.2019.07.005},
  file         = {:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Fourier at the heart of computer music From harmonic sounds to texture.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Fourier At The Heart Of Computer Music - From Harmonic Sounds To Texture.tex:TEX},
  journaltitle = {Comptes Rendus Physique},
  keywords     = {Fourier analysis, Computer music, Audio signal processing, Analyse de Fourier, Musique par ordinateur, Traitement du signal audio-numérique},
  priority     = {prio3},
  ranking      = {rank2},
  readstatus   = {read},
  url          = {https://www.sciencedirect.com/science/article/pii/S1631070519301057},
}

@Article{Hsiao2017,
  author       = {Shih-Wen Hsiao and Shih-Kai Chen and Chu-Hsuan Lee},
  title        = {Methodology for stage lighting control based on music emotions},
  issn         = {0020-0255},
  pages        = {14-35},
  volume       = {412-413},
  abstract     = {Traditionally, stage lighting regulations have required that professionally trained technicians operate the lighting equipment; however, contemporary demands for higher-quality performances require more preparation before a performance. Thus, technicians or club DJs now spend double to triple the time previously required before a show on matching the lighting control sequence musical instrument digital interface (MIDI) with the music, which is very time consuming. Thus, a methodology for automatic stage-lighting regulation would be very useful. Recently, the development of music emotion recognition (MER) and neural network algorithms has progressed significantly. Feelings related to music can be recognized and are even quantifiable using a supervised machine learning approach. In this study, a variety of music signal features from 2,087 song clips were captured, and then, a cross-validation test based on the support vector machine's (SVM) accuracy of classifying them into Thayer's emotion plane was applied to the main features related to music emotions, in order to produce linear quantitative values for describing music emotions. Music emotions and color preferences for stage lighting were subsequently studied. Using the experimental results, a support vector regression (SVR) model was trained to construct simulations. To increase the realism of the simulations, we developed an automatic music segment detection methodology based on music signal intensity to capture the different music strengths and feelings in each segment. Furthermore, music genres were studied as a factor for developing a comprehensive automatic stage lighting system based on feelings, genre, and the intensity of each segment of music.},
  date         = {2017},
  doi          = {https://doi.org/10.1016/j.ins.2017.05.026},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Methodology for stage lighting control based on music emotions.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Methodology for Stage Lighting Control Based on Music Emotions.tex:TEX},
  journaltitle = {Inform Sci},
  keywords     = {Automatic stage-lighting regulation, Music emotion recognition, Lighting color regulation based on music emotions and genre, Support vector regression (SVR), Automatic music segment detection},
  ranking      = {rank5},
  url          = {https://www.sciencedirect.com/science/article/pii/S0020025517307351},
}

@Article{Zangerle2018,
  author       = {E. {Zangerle} and C. {Chen} and M. {Tsai} and Y. {Yang}},
  title        = {Leveraging Affective Hashtags for Ranking Music Recommendations},
  pages        = {1-1},
  date         = {2018},
  doi          = {10.1109/TAFFC.2018.2846596},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Leveraging Affective Hashtags for Ranking Music Recommendations.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Leveraging Affective Hashtags for Ranking Music Recommendations.tex:TEX},
  journaltitle = {IEEE Transactions on Affective Computing},
  readstatus   = {read},
}

@Article{9333553,
  author  = {Castillo, Jaime Ramirez and Flores, M. Julia},
  journal = {IEEE Access},
  title   = {Web-Based Music Genre Classification for Timeline Song Visualization and Analysis},
  year    = {2021},
  issn    = {2169-3536},
  pages   = {18801-18816},
  volume  = {9},
  doi     = {10.1109/ACCESS.2021.3053864},
  file    = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Web-based Music Genre Classification for Timeline Song Visualization and Analysis.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Web-based Music Genre Classification for Timeline Song Visualization and Analysis.tex:TEX},
}

@Article{Pennycook1985,
  author       = {Pennycook, Bruce},
  title        = {Computer-Music Interfaces: A Survey},
  pages        = {267-289},
  volume       = {17},
  date         = {1985-06},
  doi          = {10.1145/4468.4470},
  file         = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/SURVEY/Computer Music Interfaces A Survey.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Computer-Music Interfaces - A Survey.tex:TEX},
  journaltitle = {ACM Comput. Surv.},
  readstatus   = {skimmed},
}

@Article{8327886,
  author  = {R. {Panda} and R. {Malheiro} and R. P. {Paiva}},
  journal = {IEEE Transactions on Affective Computing},
  title   = {Novel Audio Features for Music Emotion Recognition},
  year    = {2020},
  number  = {4},
  pages   = {614-626},
  volume  = {11},
  doi     = {10.1109/TAFFC.2018.2820691},
  file    = {:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Novel Audio Features for Music Emotion Recognition.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Novel Audio Features for Music Emotion Recognition.tex:TEX},
}

@Article{Rumiantcev2020,
  author    = {Mikhail Rumiantcev and Oleksiy Khriyenko},
  journal   = {Proceedings of the XXth Conference of Open Innovations Association FRUCT},
  title     = {Emotion Based Music Recommendation System},
  year      = {2020},
  issn      = {2305-7254},
  number    = {2},
  pages     = {639--645},
  volume    = {26},
  abstract  = {<p>Nowadays, music platforms provide easy access to large amounts of music. They are working continuously to improve music organization and search management thereby addressing the problem of choice and simplify exploring new music pieces. Recommendation systems gain more and more popularity and help people to select appropriate music for all occasions. However, there is still a gap in personalization and emotions driven recommendations. Music has great influence on humans and is widely used for relaxing, mood regulation, destruction from stress and diseases, to maintain mental and physical work. There is a wide range of clinical settings and practices in music therapy for the wellbeing support. This paper will present design of the personalized music recommendation system, driven by listener feelings, emotions and activity contexts. With combination of artificial intelligent technologies and generalized music therapy approaches, recommendation system is targeted to help people with...},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Emotion Based Music Recommendation System.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Emotion Based Music Recommendation System .tex:TEX},
  keywords  = {Artificial Intelligence ; Machine Learning ; Music Curation ; Recommendation System},
  language  = {eng},
  publisher = {FRUCT},
  url       = {https://doaj.org/article/21bcc31afa7a4adea0c95fc4e82f68db},
}

@Article{Zhang2013,
  author   = {Kejun Zhang and Shouqian Sun},
  journal  = {Neurocomputing},
  title    = {Web music emotion recognition based on higher effective gene expression programming},
  year     = {2013},
  issn     = {0925-2312},
  note     = {Learning for Scalable Multimedia Representation},
  pages    = {100-106},
  volume   = {105},
  abstract = {In the study, we present a higher effective algorithm, called revised gene expression programming (RGEP), to construct the model for music emotion recognition. Our main contributions are as follows: firstly, we describe the basic mechanisms of music emotion recognition and introduce gene expression programming (GEP) to deal with the model construction for music emotion recognition. Secondly, we present RGEP based on backward-chaining evolutionary algorithm and use GEP, RGEP, and SVM to construct the models for music emotion recognition separately, the results show that the models obtained by SVM, GEP, and RGEP are satisfactory and well confirm the experimental values. Finally, we report the comparison of these models, and we find that the model obtained by RGEP outperforms classification accuracy of the model by GEP and takes almost 15% less processing time of GEP and even half processing time of SVM, which offers a new efficient way for solving music emotion recognition problems; moreover, because processing time is essential for the problem of large scale music information retrieval, therefore, RGEP might prompt the development of the music information retrieval technology.},
  doi      = {https://doi.org/10.1016/j.neucom.2012.06.041},
  file     = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Web music emotion recognition based on higher effective gene expression programming.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 1/Fichamento Web Music Emotion Recognition Based on Higher Effective Gene Expression Programming.tex:TEX},
  keywords = {Music emotion recognition, Evolutionary algorithm, Gene expression programming, Support vector machine, Music information retrieval},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231212007035},
}

@InProceedings{8405505,
  author    = {S. {Sharma} and P. {Fulzele} and I. {Sreedevi}},
  booktitle = {2018 IEEE Symposium on Computer Applications Industrial Electronics (ISCAIE)},
  title     = {Novel hybrid model for music genre classification based on support vector machine},
  year      = {2018},
  pages     = {395-400},
  doi       = {10.1109/ISCAIE.2018.8405505},
  file      = {Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 2/Fichamento Novel hybrid model for music genre classification based on support vector machine.tex:TEX;Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Novel hybrid model for music genre classification based on support vector machine.pdf:PDF},
}

@Article{Zampoglou2014,
  author     = {Zampoglou, Markos and Malamos, Athanasios G},
  journal    = {The new review of hypermedia and multimedia},
  title      = {Music information retrieval in compressed audio files: a survey},
  year       = {2014},
  issn       = {1361-4568},
  number     = {3},
  pages      = {189--206},
  volume     = {20},
  abstract   = {<p>In this paper, we present an organized survey of the existing literature on music information retrieval systems in which descriptor features are extracted directly from the compressed audio files, without prior decompression to pulse-code modulation format. Avoiding the decompression step and utilizing the readily available compressed-domain information can significantly lighten the computational cost of a music information retrieval system, allowing application to large-scale music databases. We identify a number of systems relying on compressed-domain information and form a systematic classification of the features they extract, the retrieval tasks they tackle and the degree in which they achieve an actual increase in the overall speed-as well as any resulting loss in accuracy. Finally, we discuss recent developments in the field, and the potential research directions they open toward ultra-fast, scalable systems.</p>},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Music information retrieval in compressed audio files a survey.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Music information retrieval in compressed audio files a survey.tex:TEX},
  keywords   = {Music Information Retrieval ; Audio Compression ; Mpeg 7 ; Computer Science},
  language   = {eng},
  publisher  = {Taylor & Francis},
  readstatus = {read},
  url        = {http://www.tandfonline.com/doi/abs/10.1080/13614568.2014.889223},
}

@Article{Choi2018,
  author     = {Choi, Keunwoo and Fazekas, György and Cho, Kyunghyun and Sandler, Mark},
  journal    = {arXiv.org},
  title      = {A Tutorial on Deep Learning for Music Information Retrieval},
  year       = {2018},
  issn       = {2331-8422},
  abstract   = {Following their success in Computer Vision and other areas, deep learning techniques have recently become widely adopted in Music Information Retrieval (MIR) research. However, the majority of works aim to adopt and assess methods that have been shown to be effective in other domains, while there is still a great need for more original research focusing on music primarily and utilising musical knowledge and insight. The goal of this paper is to boost the interest of beginners by providing a comprehensive tutorial and reducing the barriers to entry into deep learning for MIR. We lay out the basic principles and review prominent works in this hard to navigate the field. We then outline the network structures that have been successful in MIR problems and facilitate the selection of building blocks for the problems at hand. Finally, guidelines for new tasks and some advanced topics in deep learning are discussed to stimulate new research in this fascinating field.},
  address    = {Ithaca},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/A Tutorial on Deep Learning for Music Information Retrieval.pdf:PDF;FICHAMENTO:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento A Tutorial on Deep Learning for Music Information Retrieval.tex:TEX},
  keywords   = {Information Retrieval ; Music ; Computer Vision ; Machine Learning ; Information Retrieval ; Domains ; Computer Vision and Pattern Recognition ; Sound},
  language   = {eng},
  publisher  = {Cornell University Library, arXiv.org},
  readstatus = {read},
  url        = {http://search.proquest.com/docview/2072233560/},
}

@Article{Benetos2019,
  author     = {Benetos, Emmanouil and Dixon, Simon and Duan, Zhiyao and Ewert, Sebastian},
  journal    = {IEEE Signal Processing Magazine},
  title      = {Automatic Music Transcription: An Overview},
  year       = {2019},
  number     = {1},
  pages      = {20-30},
  volume     = {36},
  doi        = {10.1109/MSP.2018.2869928},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Automatic Music Transcription - An Overview.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Automatic Music Transcription - An Overview.tex:TEX},
  readstatus = {read},
}

@Article{Alvarado2016,
  author     = {Alvarado, Pablo A. and Stowell, Dan},
  title      = {Gaussian Processes for Music Audio Modelling and Content Analysis},
  year       = {2016},
  abstract   = {Real music signals are highly variable, yet they have strong statistical structure. Prior information about the underlying physical mechanisms by which sounds are generated and rules by which complex sound structure is constructed (notes, chords, a complete musical score), can be naturally unified using Bayesian modelling techniques. Typically algorithms for Automatic Music Transcription independently carry out individual tasks such as multiple-F0 detection and beat tracking. The challenge remains to perform joint estimation of all parameters. We present a Bayesian approach for modelling music audio, and content analysis. The proposed methodology based on Gaussian processes seeks joint estimation of multiple music concepts by incorporating into the kernel prior information about non-stationary behaviour, dynamics, and rich spectral content present in the modelled music signal. We illustrate the benefits of this approach via two tasks: pitch estimation, and inferring missing segments in a polyphonic audio recording.},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/GAUSSIAN PROCESSES FOR MUSIC AUDIO MODELLING AND CONTENT ANALYSIS.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Gaussian Processes for Music Audio Modelling and Content Analysis.tex:TEX},
  keywords   = {Statistics - Machine Learning ; Computer Science - Sound},
  readstatus = {read},
}

@Article{Chang2017,
  author   = {Chang, Sungkyun and Lee, Juheon and Choe, Sang Keun and Lee, Kyogu},
  title    = {Audio Cover Song Identification using Convolutional Neural Network},
  year     = {2017},
  abstract = {In this paper, we propose a new approach to cover song identification using a CNN (convolutional neural network). Most previous studies extract the feature vectors that characterize the cover song relation from a pair of songs and used it to compute the (dis)similarity between the two songs. Based on the observation that there is a meaningful pattern between cover songs and that this can be learned, we have reformulated the cover song identification problem in a machine learning framework. To do this, we first build the CNN using as an input a cross-similarity matrix generated from a pair of songs. We then construct the data set composed of cover song pairs and non-cover song pairs, which are used as positive and negative training samples, respectively. The trained CNN outputs the probability of being in the cover song relation given a cross-similarity matrix generated from any two pieces of music and identifies the cover song by ranking on the probability. Experimental results show that...},
  file     = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Audio Cover Song Identification using Convolutional Neural Network.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento  Audio Cover Song Identification using Convolutional Neural Network.tex:TEX},
  keywords = {Computer Science - Sound ; Computer Science - Artificial Intelligence ; Computer Science - Machine Learning ; Electrical Engineering And Systems Science - Audio And Speech Processing},
}

@Misc{Typke2005,
  author     = {Rainer Typke and Frans Wiering and Remco C. Veltkamp},
  title      = {A Survey Of Music Information Retrieval Systems.},
  year       = {2005},
  abstract   = {[TODO] Add abstract here....},
  file       = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/A SURVEY OF MUSIC INFORMATION RETRIEVAL SYSTEMS.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento A Survey Of Music Information Retrieval Systems..tex:TEX},
  publisher  = {Zenodo},
  readstatus = {read},
}

@Article{lee_chiang_lin_lin_tai_2016,
  author    = {Lee, Yuan-Shan and Chiang, Yen-Lin and Lin, Pei-Rung and Lin, Chang-Hung and Tai, Tzu-Chiang},
  journal   = {APSIPA Transactions on Signal and Information Processing},
  title     = {Robust and efficient content-based music retrieval system},
  year      = {2016},
  pages     = {e4},
  volume    = {5},
  doi       = {10.1017/ATSIP.2016.4},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Robust and efficient content-based music retrieval system.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Robust and efficient content-based music retrieval system.tex:TEX},
  publisher = {Cambridge University Press},
}

@Article{1021072,
  author  = {Tzanetakis, G. and Cook, P.},
  journal = {IEEE Transactions on Speech and Audio Processing},
  title   = {Musical genre classification of audio signals},
  year    = {2002},
  number  = {5},
  pages   = {293-302},
  volume  = {10},
  doi     = {10.1109/TSA.2002.800560},
  file    = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Musical Genre Classification of Audio Signals.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Musical genre classification of audio signals.tex:TEX},
}

@Article{Siddavatam2020,
  author    = {Siddavatam, Irfan and Dalvi, Ashwini and Gupta, Dipen and Farooqui, Zaid and Chouhan, Mihir},
  journal   = {International Journal of Information Engineering and Electronic Business},
  title     = {Multi Genre Music Classification and Conversion System},
  year      = {2020},
  issn      = {20749023},
  number    = {1},
  pages     = {30},
  volume    = {10},
  abstract  = {Artificial Intelligence (AI) has a huge scope in automating, stream- lining, and increasing productivity of Music Industry. Here, we look upon AI based techniques for classifying a piece of music into multiple genres and then later converting it into another user-specified genre. Plenty of work has been done in classification, but using traditional machine learning models which are limited in term of accuracy and rely heavily on features to train the model. The novelty of this work lies in its attempt to covert genre of music from one type to another. This paper focuses on classification achieved by using a model trained via Convolutional Neural Networks. Conversion of music genre, a relatively less worked upon field has been discussed in this paper along with details of implementation. For Conversion, we initially convert the input file to spectrogram. A database of all genre is maintained at all times and a random file from user selected genre is also converted to spectrogram. Later,...},
  address   = {Hong Kong},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Multi Genre Music Classification and Conversion.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 3/Fichamento Multi Genre Music Classification and Conversion System.tex:TEX},
  keywords  = {Conversion ; Music ; Genre ; Music ; Signal Processing ; Classification ; Audio Data ; Artificial Intelligence ; Machine Learning ; Artificial Intelligence ; Spectrograms ; Artificial Neural Networks ; End Users ; Music ; Artificial Intelligence ; Genre ; Music Classification ; Music Conversion ; Convolution Neural Network (CNN)},
  language  = {eng},
  publisher = {Modern Education and Computer Science Press},
  url       = {http://search.proquest.com/docview/2350538500/},
}

@Article{Serra2009,
  author    = {Joan Serr{\`{a}} and Xavier Serra and Ralph G Andrzejak},
  journal   = {New Journal of Physics},
  title     = {Cross recurrence quantification for cover song identification},
  year      = {2009},
  month     = {sep},
  number    = {9},
  pages     = {093017},
  volume    = {11},
  abstract  = {There is growing evidence that nonlinear time series analysis techniques can be used to successfully characterize, classify, or process signals derived from real-world dynamics even though these are not necessarily deterministic and stationary. In the present study, we proceed in this direction by addressing an important problem our modern society is facing, the automatic classification of digital information. In particular, we address the automatic identification of cover songs, i.e. alternative renditions of a previously recorded musical piece. For this purpose, we here propose a recurrence quantification analysis measure that allows the tracking of potentially curved and disrupted traces in cross recurrence plots (CRPs). We apply this measure to CRPs constructed from the state space representation of musical descriptor time series extracted from the raw audio signal. We show that our method identifies cover songs with a higher accuracy as compared to previously published techniques. Beyond the particular application proposed here, we discuss how our approach can be useful for the characterization of a variety of signals from different scientific disciplines. We study coupled Rössler dynamics with stochastically modulated mean frequencies as one concrete example to illustrate this point.},
  doi       = {10.1088/1367-2630/11/9/093017},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Cross recurrence quantification for cover song identification.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 4/Fichamento Cross recurrence quantification for cover song identification.tex:TEX},
  publisher = {{IOP} Publishing},
  url       = {https://doi.org/10.1088/1367-2630/11/9/093017},
}

@Article{8678825,
  author  = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schlüter, Jan and Chang, Shuo-Yiin and Sainath, Tara},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  title   = {Deep Learning for Audio Signal Processing},
  year    = {2019},
  number  = {2},
  pages   = {206-219},
  volume  = {13},
  doi     = {10.1109/JSTSP.2019.2908700},
  file    = {Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 4/Fichamento Deep Learning for Audio Signal Processing.tex:TEX;Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Deep Learning for Audio Signal Processing.pdf:PDF},
}

@InProceedings{8257086,
  author    = {Ponighzwa, R. Mochammad Faris and Sarno, Riyanarto and Sunaryono, Dwi},
  booktitle = {2017 3rd International Conference on Science in Information Technology (ICSITech)},
  title     = {Cover song recognition based on MPEG-7 audio features},
  year      = {2017},
  pages     = {59-65},
  doi       = {10.1109/ICSITech.2017.8257086},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Cover song recognition based on MPEG-7 audio features.pdf:PDF;Fichamento:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Cover song recognition based on MPEG-7 audio features.pdf:PDF},
}

@InProceedings{10.1145/3077136.3080680,
  author    = {Cheng, Yao and Chen, Xiaoou and Yang, Deshun and Xu, Xiaoshuo},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Effective Music Feature NCP: Enhancing Cover Song Recognition with Music Transcription},
  year      = {2017},
  address   = {New York, NY, USA},
  pages     = {925–928},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '17},
  abstract  = {Chroma is a widespread feature for cover song recognition, as it is robust against non-tonal components and independent of timbre and specific instruments. However, Chroma is derived from spectrogram, thus it provides a coarse approximation representation of musical score. In this paper, we proposed a similar but more effective feature Note Class Profile (NCP) derived with music transcription techniques. NCP is a multi-dimensional time serie, each column of which denotes the energy distribution of 12 note classes. Experimental results on benchmark datasets demonstrated its superior performance over existing music features. In addition, NCP feature can be enhanced further with the development of music transcription techniques. The source code can be found in github1.},
  doi       = {10.1145/3077136.3080680},
  file      = {Artigo:C\:/Users/prosp/Documents/GitHub/TCC-I/BIBLIOTECA/Effective Music Feature NCP Enhancing Cover Song Recognition with Music Transcription.pdf:PDF;Artigo:C\:/Users/Próspero/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 4/Fichamento Effective Music Feature NCP - Enhancing Cover Song Recognition with Music Transcription.tex:TEX},
  isbn      = {9781450350228},
  keywords  = {cover song recognition, dynamic programming},
  location  = {Shinjuku, Tokyo, Japan},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3077136.3080680},
}

@inproceedings{yesiler2019,
    author = "Furkan Yesiler and Chris Tralie and Albin Correya and Diego F. Silva and Philip Tovstogan and Emilia G{\'{o}}mez and Xavier Serra",
    title = "{Da-TACOS}: A Dataset for Cover Song Identification and Understanding",
    booktitle = "Proc. of the 20th Int. Soc. for Music Information Retrieval Conf. (ISMIR)",
    year = "2019",
    pages = "327--334",
    address = "Delft, The Netherlands"
}

@Article{8392419,
  author  = {Silva, Diego F. and Yeh, Chin-Chia M. and Zhu, Yan and Batista, Gustavo E. A. P. A. and Keogh, Eamonn},
  journal = {IEEE Transactions on Multimedia},
  title   = {Fast Similarity Matrix Profile for Music Analysis and Exploration},
  year    = {2019},
  number  = {1},
  pages   = {29-38},
  volume  = {21},
  doi     = {10.1109/TMM.2018.2849563},
  file    = {Artigo:C\:/Users/Próspero/Documents/GitHub/TCC-I/BIBLIOTECA/Fast Similarity Matrix Profile for Music Analysis and Exploration.pdf:PDF;Fichamento:C\:/Users/Próspero/Documents/GitHub/TCC-I/AUXILIARES/Fichamentos/Entregues/Entrega 4/Fichamento Fast Similarity Matrix Profile for Music Analysis and Exploration.tex:TEX},
}

@InProceedings{Cabral2005ImpactOD,
  author = {G. Cabral and J. Briot and F. Pachet},
  title  = {Impact of Distance in Pitch Class Profile Computation},
  year   = {2005},
  file   = {Artigo:C\:/Users/Próspero/Documents/GitHub/TCC-I/BIBLIOTECA/Impact of Distance in Pitch Class Profile Computation.pdf:PDF},
}

@InProceedings{7952229,
  author    = {Seetharaman, Prem and Rafii, Zafar},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Cover song identification with 2D Fourier Transform sequences},
  year      = {2017},
  pages     = {616-620},
  doi       = {10.1109/ICASSP.2017.7952229},
  file      = {Artigo:C\:/Users/Próspero/Documents/GitHub/TCC-I/BIBLIOTECA/Cover song identification with 2D Fourier Transform sequences.pdf:PDF},
}

@Comment{jabref-meta: databaseType:bibtex;}
